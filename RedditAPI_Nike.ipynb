{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nBicPimE2LM",
        "outputId": "abb2ca63-8683-43f8-ef97-712b6f786967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n",
            "Downloading praw-7.8.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.0 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install praw\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18DGGZK_SMoZ",
        "outputId": "6d9d24eb-f713-406c-9192-ea0eaf2c8933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
            "Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.3 h11-0.14.0 starlette-0.41.1 uvicorn-0.32.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HI6acetSQbR",
        "outputId": "98c5a67b-4b44-4a08-d575-f4d7acf97654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaUBjTYUSl_y",
        "outputId": "9aeb83b3-98f6-436c-a023-baa843f3a81e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A54-FMCETR6r",
        "outputId": "d429bdc4-e6a5-40c8-adac-2241bb0ad60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9o0hqadTmXq",
        "outputId": "c467f6fa-64e8-4d26-efd5-19ed9a2ce214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REDDIT_CLIENT_ID = '########'\n",
        "REDDIT_CLIENT_SECRET = '##########'\n",
        "REDDIT_USER_AGENT = '##########'"
      ],
      "metadata": {
        "id": "5ZKjj_YnUBa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "khzWmlEeWteo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def authenticate_reddit():\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=REDDIT_CLIENT_ID,\n",
        "        client_secret=REDDIT_CLIENT_SECRET,\n",
        "        user_agent=REDDIT_USER_AGENT\n",
        "    )\n",
        "    return reddit\n",
        "\n",
        "def fetch_posts(reddit, subreddit_name, keyword, limit=100):\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    search_query = f'{keyword}'\n",
        "    posts = []\n",
        "    for submission in subreddit.search(search_query, limit=limit):\n",
        "        posts.append({\n",
        "            'id': submission.id,\n",
        "            'title': submission.title,\n",
        "            'selftext': submission.selftext,\n",
        "            'score': submission.score,\n",
        "            'created_utc': submission.created_utc,\n",
        "            'url': submission.url,\n",
        "            'num_comments': submission.num_comments\n",
        "        })\n",
        "    return pd.DataFrame(posts)\n",
        "\n",
        "def main():\n",
        "    reddit = authenticate_reddit()\n",
        "    nike_related_subreddits = ['nike', 'sneakers', 'running']  # Adding relevant subreddits\n",
        "    keyword = 'Nike Shoes'\n",
        "    all_posts = pd.DataFrame()\n",
        "\n",
        "    for subreddit in nike_related_subreddits:\n",
        "        print(f'Fetching posts from r/{subreddit}...')\n",
        "        df = fetch_posts(reddit, subreddit, keyword, limit=200)\n",
        "        df['subreddit'] = subreddit\n",
        "        all_posts = pd.concat([all_posts, df], ignore_index=True)\n",
        "\n",
        "    all_posts.to_csv('nike_reddit_posts.csv', index=False)\n",
        "    print('Data collection complete. Saved to nike_reddit_posts.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E394autLWvH1",
        "outputId": "f2a99b1e-ca4f-4ed2-85ad-8718c0b79a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts from r/nike...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts from r/sneakers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching posts from r/running...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection complete. Saved to nike_reddit_posts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_reddit_data.csv' with your actual file path\n",
        "df = pd.read_csv('nike_reddit_posts.csv')\n",
        "\n",
        "# Inspect the data\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abcml-1rbpEp",
        "outputId": "ec7fbfcf-f9e9-4025-98af-a9d47ac154e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                                              title  \\\n",
            "0  1e70ard               Most comfortable Nike shoes you own?   \n",
            "1   lan856  Yea I was that mom that messaged Nike asking q...   \n",
            "2  1ffrb6s  What do people think of the Nike PS8? Anyone s...   \n",
            "3  1as75r6  I had a question about a Nike shoe so I google...   \n",
            "4   xvwx7l  Is anybody out there bothered by going for Nik...   \n",
            "\n",
            "                                            selftext  score   created_utc  \\\n",
            "0  I'm a big fan of Adidas UltraBoost due to thei...     24  1.721384e+09   \n",
            "1                                                NaN   1494  1.612240e+09   \n",
            "2                                                NaN     45  1.726223e+09   \n",
            "3                                                NaN    275  1.708086e+09   \n",
            "4                                                NaN    157  1.664934e+09   \n",
            "\n",
            "                                                 url  num_comments subreddit  \n",
            "0  https://www.reddit.com/r/Nike/comments/1e70ard...           134      nike  \n",
            "1                https://i.redd.it/u33n4f2cqze61.jpg            66      nike  \n",
            "2               https://i.redd.it/4o3kgiqsyjod1.jpeg            48      nike  \n",
            "3               https://i.redd.it/21rov5rnvxic1.jpeg            47      nike  \n",
            "4                https://i.redd.it/dze1prvz4wr91.jpg           169      nike  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk vaderSentiment pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AoN3PezpOnp",
        "outputId": "16d01a9c-02f9-4864-ece6-d9c067662e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file containing Reddit posts\n",
        "reddit_data = pd.read_csv('nike_reddit_posts.csv')\n",
        "\n",
        "# Preview the data to understand its structure\n",
        "print(reddit_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHRiD1UxpalZ",
        "outputId": "494eed69-a836-488e-e587-c0ee702b06af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                                              title  \\\n",
            "0  1e70ard               Most comfortable Nike shoes you own?   \n",
            "1   lan856  Yea I was that mom that messaged Nike asking q...   \n",
            "2  1ffrb6s  What do people think of the Nike PS8? Anyone s...   \n",
            "3  1as75r6  I had a question about a Nike shoe so I google...   \n",
            "4   xvwx7l  Is anybody out there bothered by going for Nik...   \n",
            "\n",
            "                                            selftext  score   created_utc  \\\n",
            "0  I'm a big fan of Adidas UltraBoost due to thei...     24  1.721384e+09   \n",
            "1                                                NaN   1494  1.612240e+09   \n",
            "2                                                NaN     45  1.726223e+09   \n",
            "3                                                NaN    275  1.708086e+09   \n",
            "4                                                NaN    157  1.664934e+09   \n",
            "\n",
            "                                                 url  num_comments subreddit  \n",
            "0  https://www.reddit.com/r/Nike/comments/1e70ard...           134      nike  \n",
            "1                https://i.redd.it/u33n4f2cqze61.jpg            66      nike  \n",
            "2               https://i.redd.it/4o3kgiqsyjod1.jpeg            48      nike  \n",
            "3               https://i.redd.it/21rov5rnvxic1.jpeg            47      nike  \n",
            "4                https://i.redd.it/dze1prvz4wr91.jpg           169      nike  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords (if you haven't already)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get a list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean and preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove stopwords (optional, depends on the use case)\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to your data\n",
        "reddit_data['Cleaned_title'] = reddit_data['title'].apply(preprocess_text)\n",
        "\n",
        "# Preview the cleaned data\n",
        "print(reddit_data[['title', 'Cleaned_title']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eurUWSIQqISI",
        "outputId": "038f2c34-02c9-439f-98f7-a8b227a25564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  \\\n",
            "0               Most comfortable Nike shoes you own?   \n",
            "1  Yea I was that mom that messaged Nike asking q...   \n",
            "2  What do people think of the Nike PS8? Anyone s...   \n",
            "3  I had a question about a Nike shoe so I google...   \n",
            "4  Is anybody out there bothered by going for Nik...   \n",
            "\n",
            "                                       Cleaned_title  \n",
            "0                             comfortable nike shoes  \n",
            "1  yea mom messaged nike asking questions thought...  \n",
            "2  people think nike ps anyone skated personally ...  \n",
            "3       question nike shoe googled got lovely answer  \n",
            "4  anybody bothered going nike socks adidas shoes...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to calculate sentiment score\n",
        "def analyze_sentiment(text):\n",
        "    return analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "# Apply the sentiment analysis to each Reddit post\n",
        "reddit_data['Sentiment'] = reddit_data['title'].apply(analyze_sentiment)\n",
        "\n",
        "# Display the results\n",
        "print(reddit_data[['title', 'Sentiment']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUGior5Iqy-C",
        "outputId": "b6832709-cae8-4ada-b13f-073ada7ea6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  Sentiment\n",
            "0               Most comfortable Nike shoes you own?     0.5563\n",
            "1  Yea I was that mom that messaged Nike asking q...     0.8437\n",
            "2  What do people think of the Nike PS8? Anyone s...    -0.4314\n",
            "3  I had a question about a Nike shoe so I google...     0.6705\n",
            "4  Is anybody out there bothered by going for Nik...    -0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The compound score is a normalized value between -1 (most negative) and +1 (most positive), indicating the overall sentiment of the post."
      ],
      "metadata": {
        "id": "EESZ5RB9rAA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentiment(score):\n",
        "    if score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Apply the classification to each post\n",
        "reddit_data['Sentiment_Class'] = reddit_data['Sentiment'].apply(classify_sentiment)\n",
        "\n",
        "# Display the updated data with sentiment classification\n",
        "print(reddit_data[['title', 'Sentiment', 'Sentiment_Class']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTyYdRstrE0K",
        "outputId": "f61cd46a-9efd-47a3-989a-0e3cbcfbbe0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  Sentiment  \\\n",
            "0               Most comfortable Nike shoes you own?     0.5563   \n",
            "1  Yea I was that mom that messaged Nike asking q...     0.8437   \n",
            "2  What do people think of the Nike PS8? Anyone s...    -0.4314   \n",
            "3  I had a question about a Nike shoe so I google...     0.6705   \n",
            "4  Is anybody out there bothered by going for Nik...    -0.2500   \n",
            "\n",
            "  Sentiment_Class  \n",
            "0        Positive  \n",
            "1        Positive  \n",
            "2        Negative  \n",
            "3        Positive  \n",
            "4        Negative  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data.to_csv('nike_reddit_posts_with_sentiment.csv', index=False)\n",
        "print(\"Sentiment analysis completed and saved to nike_reddit_posts_with_sentiment.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD4jHroqrSxI",
        "outputId": "88530089-66b2-4c08-c269-6f3e6305c55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis completed and saved to nike_reddit_posts_with_sentiment.csv\n"
          ]
        }
      ]
    }
  ]
}